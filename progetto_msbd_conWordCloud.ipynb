{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : Analysis of the topics covered by Italian politicians on Twitter during Covid19 pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from scipy.sparse import lil_matrix\n",
    "import itertools\n",
    "import random\n",
    "from scipy import sparse as sp\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura dei dati \n",
    "dati = pd.read_csv(\"tantitweet9.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "print(\"Dimensione del dataset: {} x {}\".format(*dati.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo il nome dei politici in una lista\n",
    "nome_politici = dati[\"screen_name\"].unique()\n",
    "print(nome_politici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo una lista contenente una lista per ogni politico dei suo tweet\n",
    "tweet = []\n",
    "for nome in (nome_politici):\n",
    "        tweet.append(dati.loc[dati[\"screen_name\"] == nome][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro i tweet che sono piu lunghi di 200 caratteri\n",
    "# per ogni tweet associo l'autore e lo salvo in una lista\n",
    "tweet2 = []\n",
    "autore = []\n",
    "for i,t in enumerate(tweet):\n",
    "    lista_temp = []\n",
    "    for k in t:\n",
    "        if len(k) > 200:\n",
    "            lista_temp.append(k)\n",
    "            autore.append(nome_politici[i])\n",
    "    tweet2.append(lista_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero tweet per politico\n",
    "dim = []\n",
    "for i,t in enumerate(tweet2):\n",
    "    print(i,len(t), nome_politici[i]) \n",
    "    dim.append(len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie utilizzate per il pre-processing\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import tqdm\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che esegue le operazioni di pre-processing\n",
    "def tweet_analyzer(tweet, tokenizer, stop_words):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    tweet = re.sub(\"http\\S+\", \"\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    tweet = [token for token in tweet if token not in stop_words\n",
    "        and not token.isdigit()]\n",
    "    tweet = [token for token in tweet if len(token) > 1]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste di stopword scaricate da nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "# lista di stopword contenti nomi e partiti dei politici e altre parole senza significato\n",
    "piu_stopwords = [\"oggi\", \"ora\",\"solo\",\"cosa\",\"giuseppeconteit\", \"fatto\", \"quando\", \"poi\", \"dire\",\"essere\",\"mai\",\"prima\",\"molto\",\"albertobagnai\",\"borghi_claudio\",\n",
    "                \"sempre\",\"modo\",\"punto\",\"nulla\",\"giorni\",\"deve\",\"così\",\"può\",\"cose\",\"fa\",\"anni\",\"dobbiamo\",\"the\",\"alberto\",\"bagnai\",\"magari\",\"ecco\",\"gasparripdl\",\n",
    "                 \"and\",\"dare\",\"due\",\"tutte\",\"insieme\",\"berlusconi\",\"l'italia\",\"va\",\"bisogna\",\"meno\",\"parte\",\"ce\",\"grande\",\"oggi\",\"fare\",\"senza\",\"quindi\",\n",
    "                \"sempre\", \"grazie\", \"ancora\", \"momento\", \"ogni\", \"dopo\", \"già\", \"sempre\", \"salvini\",\"ieri\",\"altro\",\"pd\",\"lega\",\"m5s\",\"forza_italia\",\n",
    "                \"borghi_claudio\",\"italiaviva\",\"matteosalvinimi\",\"bonafede\",\"teresabellanova\",\"rinaldi_euro\",\"giorgio_gori\",\"civati\",\"matteorenzi\",\"sbonaccini\",\n",
    "                \"boccia\",\"pure\",\"invece\",\"ore\",\"tanti\",\"qui\",\"fra\",\"pdnetwork\",\"nzingaretti\",\"assembleapd\",\"zappingradio1\",\"lauraboldrini\",\"sinistra\",\"destra\",\n",
    "                \"laura\",\"boldirni\",\"tutta\",\"qualcuno\",\"emmabonino\",\"piueuropalive\",\"emma\",\"bonino\",\"piu_europa\",\"vedova\",\"simona\",\"luigidimaio\",\"centrodestra\",\n",
    "                \"roberto_fico\",\"ettore_rosato\",\"elenabonetti\",\"ferra2113\",\"robersperanza\",\"viva\",\"ep_president\",\"europa_it\",\"annaascani\",\"link\",\"serve\",\n",
    "                \"azione_it\",\"carlocalenda\",\"fdi\",\"fratelliditalia\",\"giorgiameloni\",\"zaiapresidente\",\"marattin\",\"fratelliditaiia\",\"lucianonobili\",\n",
    "                \"marcocappa_\",\"silviaromano\",\"pennacchiiiii\",\"chiaralessi\",\"rinaldi\",\"antonio\",\"paologentiloni\",\"gentiloni\",\"ass_coscioni\",\"f_boccia\",\n",
    "                \"marcocappato\",\"filomena_gallo\",\"antonio_tajani\",\"de\",\"el\",\"en\",\"msgelmini\",\"enricoletta\",\"de\",\"et\",\"les\",\"st\",\"mara_carfagna\",\"bene\",\n",
    "                \"far\",\"pfmajorino\",\"sbonaccini\",\"bendellavedova\",\"opencamera\",\"laveritaweb\",\"laveritaallesette\",\"capezzone\",\"alessiamorani\",\"marco\",\n",
    "                 \"renatobrunetta\",\"giornatadellaterra\",\"scorrettissima\",\"politicamente\",\"politicamente_scorrettissima\",\"peoplepubit\",\"gualtierieurope\",\n",
    "                \"renzi\",\"sa\",\"troppo\",\"pare\",\"volete\"]\n",
    "stopwords2 = ['ad','al','allo','ai','agli','all','agl','alla','alle','con','col','coi','da','dal','dallo','dai','dagli','dalla','dalle','di','del','dello','dei','degli','della','delle','in','nel','nello','nei','negli','nell','negl','nella','nelle','su','sul','sullo','sui','sugli','sull','sugl','sulla','sulle','per','tra','contro','io','tu','lui','lei','noi','voi','loro','mio','mia','miei','mie','tuo','tua','tuoi','tue','suo','sua','suoi','sue','nostro','nostra','nostri','nostre','vostro','vostra','vostri','vostre','mi','ti','ci','vi','lo','la','li','le','gli','ne','il','un','uno','una','ma','ed','se','perché','anche','come','dove','che','chi','cui','non','più','quale','quanto','quanti','quanta','quante','quello','quelli','quella','quelle','questo','questi','questa','queste','si','tutto','tutti','ho','hai','ha','abbiamo','avete','hanno','abbia','abbiate','abbiano','avrò','avrai','avrà','avremo','avrete','avranno','avrei','avresti','avrebbe','avremmo','avreste','avrebbero','avevo','avevi','aveva','avevamo','avevate','avevano','ebbi','avesti','ebbe','avemmo','aveste','ebbero','avessi','avesse','avessimo','avessero','avendo','avuto','avuta','avuti','avute','sono','sei','è','siamo','siete','sia','siate','siano','sarò','sarai','sarà','saremo','sarete','saranno','sarei','saresti','sarebbe','saremmo','sareste','sarebbero','ero','eri','era','eravamo','eravate','erano','fui','fosti','fu','fummo','foste','furono','fossi','fosse','fossimo','fossero','essendo','faccio','fai','facciamo','fanno','faccia','facciate','facciano','farò','farai','farà','faremo','farete','faranno','farei','faresti','farebbe','faremmo','fareste','farebbero','facevo','facevi','faceva','facevamo','facevate','facevano','feci','facesti','fece','facemmo','faceste','fecero','facessi','facesse','facessimo','facessero','facendo','sto','stai','sta','stiamo','stanno','stia','stiano','starò','starai','starà','staremo','starete','staranno','starei','staresti','starebbe','staremmo','stareste','starebbero','stavo','stavi','stava','stavamo','stavate','stavano','stetti','stesti','stette','stemmo','steste','stettero','stessi','stesse','stessimo','stessero','stando']\n",
    "LATIN_1_CHARS = [ \"'\",'e','-','-','-','-','-','-', \"'\",\"'\",'\"','\"', '\"','\"','\"','...',\"'\", \"'\", \"'\", \"'\", \"'\",\"'\",\"+\",\"-\",\"=\",\"(\",\")\",\"’\",\"‘\",\"“\",\"”\", \"»\",\"«\",\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('italian') + stopwords.words('english') + list(punctuation) + LATIN_1_CHARS + piu_stopwords + stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo la lista dei token dei tweet -- una sottolista per ogni politico\n",
    "token = []\n",
    "for t in tweet2:\n",
    "    token.append([tweet_analyzer(k, tokenizer, stop_words) for k in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = []\n",
    "for i in range(0,len(token)):\n",
    "    token2 = token2 + list(token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token per il grafico per il singolo politico\n",
    "token4 = []\n",
    "for i in range(0,len(token)):\n",
    "    token4.append(list(itertools.chain.from_iterable(token[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_salvini=token4[17]\n",
    "token_renzi=token4[18]\n",
    "token_gasparri=token4[24]\n",
    "token_boldrini=token4[3]\n",
    "token_civati=token4[28]\n",
    "token_calenda=token4[19]\n",
    "token_renzi=token4[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datogliere=[\"italia\",\"italiani\",\"governo\",\"paese\",\"stato\",\"italiano\",\"italiana\",\"serve\",\"altri\",\"gasparri\",\"gruppofisenato\",\"boldrini\",\"senato\",\n",
    "            \"forzaitalia\",\"senatostampa\"]\n",
    "token_salvini=[elem for elem in token_salvini if elem not in datogliere]\n",
    "token_gasparri=[elem for elem in token_gasparri if elem not in datogliere]\n",
    "token_civati=[elem for elem in token_civati if elem not in datogliere]\n",
    "token_calenda=[elem for elem in token_calenda if elem not in datogliere]\n",
    "token_renzi=[elem for elem in token_renzi if elem not in datogliere]\n",
    "token_boldrini=[elem for elem in token_boldrini if elem not in datogliere]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_salvini=Counter(token_salvini)\n",
    "c_gasparri=Counter(token_gasparri)\n",
    "c_boldrini=Counter(token_boldrini)\n",
    "c_civati=Counter(token_civati)\n",
    "c_renzi=Counter(token_renzi)\n",
    "c_calenda=Counter(token_calenda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per i colori delle parole nella wordcloud\n",
    "def random_color_func_dx(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * 80 / 255.0)\n",
    "    s = int(100.0 * 255.0 / 255.0) #verde\n",
    "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "def random_color_func_sx(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * 2 / 255.0)\n",
    "    s = int(100.0 * 255.0 / 255.0) #rosso\n",
    "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "def random_color_func_centrodx(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * 150 / 255.0)\n",
    "    s = int(100.0 * 255 / 255.0) #blu\n",
    "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h,s, l)\n",
    "\n",
    "def random_color_func_rosa(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * 0 / 255.0)\n",
    "    s = int(100.0 * 255 / 255.0) #rosa\n",
    "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(300,100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = plt.imread(\"figures/UntitledBlackCloud.jpg\")\n",
    "MAX_WORDS = 200\n",
    "MAX_FONT_SIZE = 100\n",
    "RELATIVE_SCALING = 0.1\n",
    "\n",
    "plt.show()\n",
    "plt.imshow(MASK)\n",
    "\n",
    "wc_tutti = WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    ").generate_from_frequencies(c_token)\n",
    "\n",
    "\n",
    "wc_salvini= WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    "    color_func=random_color_func_dx,\n",
    ").generate_from_frequencies(c_salvini)\n",
    "\n",
    "wc_gasparri= WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    "    color_func=random_color_func_centrodx,\n",
    ").generate_from_frequencies(c_gasparri)\n",
    "\n",
    "\n",
    "wc_civati = WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    "    color_func=random_color_func_sx,\n",
    ").generate_from_frequencies(c_civati)\n",
    "\n",
    "wc_boldrini = WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    "    color_func=random_color_func_rosa,\n",
    ").generate_from_frequencies(c_boldrini)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###word cloud per tutti\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.imshow(wc_tutti, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wordcloud salvini\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.imshow(wc_salvini,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud gasparri\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.imshow(wc_gasparri,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud boldrini\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.imshow(wc_boldrini,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud civati\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.imshow(wc_civati,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #conda install gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import logging\n",
    "import time\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo dei token formati da due parole \n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(token2, min_count=10)\n",
    "for idx in range(len(token2)):\n",
    "    for t in bigram[token2[idx]]:\n",
    "        if '_' in t:\n",
    "            token2[idx].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(token2)\n",
    "id2word.filter_extremes( no_below=10)\n",
    "corpus = [id2word.doc2bow(text) for text in token2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inizio = time.time()\n",
    "num_topics = 20\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           passes=20,\n",
    "                                           random_state = 142\n",
    "                                           )\n",
    "fine = time.time()\n",
    "print(\"Durata: {:.2f}s\".format(fine - inizio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuzione delle parole nei topic\n",
    "for i in range(0,num_topics):\n",
    "    print(\"Topic: \",i)\n",
    "    print(lda_model.show_topic(i,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyLDAvis\n",
    "Funzione per l'intepretazione dei topic\n",
    "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis # pip install pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizare i topic per l'interpretazione\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assegno ad ogni topic un titolo\n",
    "topic_label = {'0':'terapia intensiva','1':'covid19 - lombardia','2':'palazzo chigi','3':'paese e solidarietà','4':'aiuto alle imprese e alle famiglie',\n",
    "               '5':'volontari / finanza','6':'emergenza covid19','7':'economia (pil, miliardi, euro, ...)','8':'non definito','9':'ricorrenze',\n",
    "               '10':'crisi del lavoro','11':'governo','12':'politica','13':'contagi e tamponi','14':'personale sanitario','15':'conferenza stampa',\n",
    "               '16':'mes e europa','17':'lavoro - cassa integrazione','18':'spesa-crescita / scuola','19':'emergenza (sanitaria, economica)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo un dataframe con: id_tweet, autore, topic dominante, \n",
    "topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "         if (j == 0):\n",
    "            topics_df = topics_df.append(pd.Series([autore[i], topic_label[str(topic_num)]]), ignore_index=True)        \n",
    "         else:\n",
    "             break\n",
    "topics_df.columns = ['Autore', 'Dominant_Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_topic_politico = []\n",
    "for i,nome in enumerate(nome_politici):\n",
    "    c_topic_politico.append(Counter(topics_df.loc[topics_df[\"Autore\"] == nome][\"Dominant_Topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_topic = Counter(topics_df[\"Dominant_Topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico frequenza complessiva dei topic nei documenti\n",
    "N = num_topics\n",
    "\n",
    "plt.figure(figsize=(30, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Frequenza complessiva dei topic nei documenti\", fontsize='x-large', fontweight=1000)\n",
    "plt.barh(*zip(*c_topic.most_common(N)), color=\"gold\")\n",
    "plt.yticks(fontsize = \"x-large\")\n",
    "plt.xticks(fontsize = \"x-large\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico frequenza dei topic per singolo politico\n",
    "N = num_topics\n",
    "id_politico = 0\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Topics piu frequenti di {}\".format(nome_politici[id_politico]),fontsize='x-large', fontweight=1000)\n",
    "plt.barh(*zip(*c_topic_politico[id_politico].most_common(5)), color=\"gold\")\n",
    "plt.yticks(fontsize = \"x-large\")\n",
    "plt.xticks(fontsize = \"x-large\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=142,\n",
    "                                           #chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=token2, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 20\n",
    "max_topics = 60\n",
    "step_size = 2\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               #gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = [ '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=20)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            #for a in alpha:\n",
    "                # iterare through beta values\n",
    "                #for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, k=k, a='auto', b='auto')\n",
    "                    # Save the model results\n",
    "                    #model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    #model_results['Alpha'].append(a)\n",
    "                    #model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
